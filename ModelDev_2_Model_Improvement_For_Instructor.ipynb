{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.7"
    },
    "colab": {
      "name": "ModelDev_2_Model_Improvement_For_Instructor.ipynb",
      "provenance": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xF2xCJDr7uf2"
      },
      "source": [
        "# 0) Importing the right tools <font color='blue'> (5 min) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRuD6jOO7uf9"
      },
      "source": [
        "### <font color='red'>0.1) Import the necessary packages with their usual aliases: </font>\n",
        "\n",
        "- pandas as pd\n",
        "- numpy as np\n",
        "- seaborn as sns\n",
        "- matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nlcvIxYd7wUf",
        "outputId": "cb543308-3197-46d0-da75-8af57bb35fd5"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "%cd /content/drive/My Drive/dataset_gclass\n",
        "!ls"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/My Drive/dataset_gclass\n",
            " 201507-citibike-tripdata.csv\t     'input_data_for_modeling_purpose (1).csv'\n",
            " 201508-citibike-tripdata.csv\t      logisticregression.pkl\n",
            " data_after_feature_engineering.csv   randomforest.pkl\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "PA6V2Qpr7uf_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "859330ae-af84-4513-cc12-90ad7835bff9"
      },
      "source": [
        "from __future__ import division\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "#### IMPORT THE USUAL PACKAGES WITH THEIR ALIASES ####\n",
        "\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "%pylab inline"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Populating the interactive namespace from numpy and matplotlib\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WLK_y03-7ugD"
      },
      "source": [
        "### <font color='red'>0.2) Import the dataset from <i>'../data/input_data_for_modeling_purpose.csv'</i></font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "tamlulFs7ugF"
      },
      "source": [
        "raw_data = pd.read_csv('input_data_for_modeling_purpose (1).csv')\n",
        "#### IMPORT AND READ THE CSV DATA USING pd.read_csv() #### "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aY2gSwL7ugF"
      },
      "source": [
        "### <font color='red'>0.3) Copy the raw_data, print samples, and prepare features & labels for training & testing</font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xCSsG_9O7ugG"
      },
      "source": [
        "data = raw_data.copy()\n",
        "#### COPY THE RAW DATA WITH THE .copy() FUNCTION ####"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": false
        },
        "id": "UT9ZqI5F7ugI",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 261
        },
        "outputId": "b75779b8-c3f5-4a8c-fa22-cba854ad2037"
      },
      "source": [
        "data.head()\n",
        "#### PRINT SAMPLES USING .sample() AND CHECK THE FEATURES ####"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>tripduration</th>\n",
              "      <th>start station latitude</th>\n",
              "      <th>start station longitude</th>\n",
              "      <th>end station latitude</th>\n",
              "      <th>end station longitude</th>\n",
              "      <th>usertype</th>\n",
              "      <th>june_day</th>\n",
              "      <th>traveled_distance</th>\n",
              "      <th>average_speed</th>\n",
              "      <th>start_day_Friday</th>\n",
              "      <th>start_day_Monday</th>\n",
              "      <th>start_day_Saturday</th>\n",
              "      <th>start_day_Sunday</th>\n",
              "      <th>start_day_Thursday</th>\n",
              "      <th>start_day_Tuesday</th>\n",
              "      <th>start_day_Wednesday</th>\n",
              "      <th>is_weekend_0</th>\n",
              "      <th>is_weekend_1</th>\n",
              "      <th>start_moment_afternoon</th>\n",
              "      <th>start_moment_evening</th>\n",
              "      <th>start_moment_morning</th>\n",
              "      <th>start_moment_night</th>\n",
              "      <th>is_circle_trip_0</th>\n",
              "      <th>is_circle_trip_1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1338</td>\n",
              "      <td>40.727103</td>\n",
              "      <td>-74.002971</td>\n",
              "      <td>40.759291</td>\n",
              "      <td>-73.988597</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1</td>\n",
              "      <td>3.778532</td>\n",
              "      <td>10.166454</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>290</td>\n",
              "      <td>40.727791</td>\n",
              "      <td>-73.985649</td>\n",
              "      <td>40.722055</td>\n",
              "      <td>-73.989111</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1</td>\n",
              "      <td>0.701381</td>\n",
              "      <td>8.706796</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>634</td>\n",
              "      <td>40.735238</td>\n",
              "      <td>-74.000271</td>\n",
              "      <td>40.749718</td>\n",
              "      <td>-74.002950</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1</td>\n",
              "      <td>1.625823</td>\n",
              "      <td>9.231803</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>159</td>\n",
              "      <td>40.716059</td>\n",
              "      <td>-73.991908</td>\n",
              "      <td>40.718939</td>\n",
              "      <td>-73.992663</td>\n",
              "      <td>Subscriber</td>\n",
              "      <td>1</td>\n",
              "      <td>0.326548</td>\n",
              "      <td>7.393548</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>1233</td>\n",
              "      <td>40.734927</td>\n",
              "      <td>-73.992005</td>\n",
              "      <td>40.710451</td>\n",
              "      <td>-73.960876</td>\n",
              "      <td>Customer</td>\n",
              "      <td>1</td>\n",
              "      <td>3.780067</td>\n",
              "      <td>11.036692</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   tripduration  start station latitude  ...  is_circle_trip_0  is_circle_trip_1\n",
              "0          1338               40.727103  ...                 1                 0\n",
              "1           290               40.727791  ...                 1                 0\n",
              "2           634               40.735238  ...                 1                 0\n",
              "3           159               40.716059  ...                 1                 0\n",
              "4          1233               40.734927  ...                 1                 0\n",
              "\n",
              "[5 rows x 24 columns]"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OSNrKryR7ugJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f9d14d0-f829-46b8-e0dd-f2f7c4a95a67"
      },
      "source": [
        "data.shape"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(935588, 24)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BijtA9nU7ugL"
      },
      "source": [
        "labels = np.array(data.usertype)\n",
        "del data['usertype']\n",
        "features = np.array(data)"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0JTNnNKt7ugM"
      },
      "source": [
        "import sklearn\n",
        "from sklearn.preprocessing import label_binarize\n",
        "binarized_labels = label_binarize(labels, classes=['Customer', 'Subscriber']).ravel() \n",
        "#### BINARIZE THE LABELS OF THE DATASET, AND RAVEL THE RESULT USING .ravel() ####\n",
        "                   #### Subscriber will be label 1, Customer label 0 ####\n",
        "                   #### This will be a binary classifiation problem ####"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QXx8fdwv7ugN"
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(features, binarized_labels, test_size=0.3, random_state = 27)\n",
        "#### SPLIT BETWEEN TRAIN AND TEST ####"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B3_t-XWU7ugO"
      },
      "source": [
        "# 1) Model Selection <font color='blue'> (40 min) </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i4F2ecAg7ugO",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7da7aa8a-219b-4050-d4ee-835c3a152156"
      },
      "source": [
        "!pip install -U scikit-learn"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (1.0.1)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (3.0.0)\n",
            "Requirement already satisfied: numpy>=1.14.6 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.19.5)\n",
            "Requirement already satisfied: scipy>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.4.1)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn) (1.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jk-1IK9y7ugP"
      },
      "source": [
        "### <font color='red'>1.1) Logistic Regression </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v6pd4PDQ7ugQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3c043e95-5e99-42f6-e9c9-7ac88c4d96bb"
      },
      "source": [
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.model_selection import cross_val_score\n",
        "lr = LogisticRegression()\n",
        "score = cross_val_score(lr, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Logistic Regression Average Score: ', score.mean())"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.81987693 0.81759128 0.81626884]\n",
            "Logistic Regression Average Score:  0.8179123479345535\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5u2Ct6Hr7ugR"
      },
      "source": [
        "### <font color='red'>1.2) Nearest Neighbors </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JRthjjLy7ugS",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "28020564-544b-47e6-bacc-d711557c065e"
      },
      "source": [
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "knn = KNeighborsClassifier()\n",
        "score = cross_val_score(knn, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('KNN Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.73499312 0.73709919 0.73326569]\n",
            "KNN Average Score:  0.735119333721233\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ydLfPapc7ugT"
      },
      "source": [
        "### <font color='red'>1.3) Gaussian Naive Bayes </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qDEmy5Yp7ugT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "01557387-d5cd-48a9-9a51-3efd38004cf8"
      },
      "source": [
        "from sklearn.naive_bayes import GaussianNB\n",
        "nb = GaussianNB()\n",
        "score = cross_val_score(nb, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Naive Bayes Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.77463431 0.77148516 0.77332766]\n",
            "Naive Bayes Average Score:  0.7731490439211871\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YZ_sFUQ07ugU"
      },
      "source": [
        "### <font color='red'>1.4) Neural Network </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PT2WoSTE7ugU",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "436289ed-4fe6-495f-b40a-b9777f98a6f1"
      },
      "source": [
        "from sklearn.neural_network import MLPClassifier\n",
        "nn = MLPClassifier(solver='lbfgs', hidden_layer_sizes=(5, 2), random_state=1)\n",
        "score = cross_val_score(nn, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Neural Network Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.5 0.5 0.5]\n",
            "Neural Network Average Score:  0.5\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1F1i0LtE7ugV"
      },
      "source": [
        "### <font color='red'>1.5) Decision Tree </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "D_z3N34M7ugV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4cbf8ed-8d14-4e46-9f91-3a5342170a60"
      },
      "source": [
        "from sklearn import tree\n",
        "dt = tree.DecisionTreeClassifier()\n",
        "score = cross_val_score(dt, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Decision Tree Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.69177409 0.69372959 0.69505664]\n",
            "Decision Tree Average Score:  0.693520104837015\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jg3-sBXv7ugW"
      },
      "source": [
        "### <font color='red'>1.6) Random Forest </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "T2zjdg4s7ugX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c5a5510-9e32-493a-95d6-3ce5e48f2956"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 10)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.8284556  0.83002625 0.82745228]\n",
            "Random Forest Average Score:  0.8286447112061053\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nWeZZtJ77ugX"
      },
      "source": [
        "# 2) Hyperparameter Tuning <font color='blue'> (45 min) </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trLUhbpL7ugY"
      },
      "source": [
        "### <font color='red'>2.1) Please use help function to have a better understanding about RandomForestClassifier and hyperparameters that can be tuned </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "jupyter": {
          "outputs_hidden": true
        },
        "scrolled": true,
        "id": "QGHvf1Hs7ugY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f429478d-cda8-4760-d17b-46d2096d0fc5"
      },
      "source": [
        "help(RandomForestClassifier)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Help on class RandomForestClassifier in module sklearn.ensemble._forest:\n",
            "\n",
            "class RandomForestClassifier(ForestClassifier)\n",
            " |  RandomForestClassifier(n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
            " |  \n",
            " |  A random forest classifier.\n",
            " |  \n",
            " |  A random forest is a meta estimator that fits a number of decision tree\n",
            " |  classifiers on various sub-samples of the dataset and uses averaging to\n",
            " |  improve the predictive accuracy and control over-fitting.\n",
            " |  The sub-sample size is controlled with the `max_samples` parameter if\n",
            " |  `bootstrap=True` (default), otherwise the whole dataset is used to build\n",
            " |  each tree.\n",
            " |  \n",
            " |  Read more in the :ref:`User Guide <forest>`.\n",
            " |  \n",
            " |  Parameters\n",
            " |  ----------\n",
            " |  n_estimators : int, default=100\n",
            " |      The number of trees in the forest.\n",
            " |  \n",
            " |      .. versionchanged:: 0.22\n",
            " |         The default value of ``n_estimators`` changed from 10 to 100\n",
            " |         in 0.22.\n",
            " |  \n",
            " |  criterion : {\"gini\", \"entropy\"}, default=\"gini\"\n",
            " |      The function to measure the quality of a split. Supported criteria are\n",
            " |      \"gini\" for the Gini impurity and \"entropy\" for the information gain.\n",
            " |      Note: this parameter is tree-specific.\n",
            " |  \n",
            " |  max_depth : int, default=None\n",
            " |      The maximum depth of the tree. If None, then nodes are expanded until\n",
            " |      all leaves are pure or until all leaves contain less than\n",
            " |      min_samples_split samples.\n",
            " |  \n",
            " |  min_samples_split : int or float, default=2\n",
            " |      The minimum number of samples required to split an internal node:\n",
            " |  \n",
            " |      - If int, then consider `min_samples_split` as the minimum number.\n",
            " |      - If float, then `min_samples_split` is a fraction and\n",
            " |        `ceil(min_samples_split * n_samples)` are the minimum\n",
            " |        number of samples for each split.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_samples_leaf : int or float, default=1\n",
            " |      The minimum number of samples required to be at a leaf node.\n",
            " |      A split point at any depth will only be considered if it leaves at\n",
            " |      least ``min_samples_leaf`` training samples in each of the left and\n",
            " |      right branches.  This may have the effect of smoothing the model,\n",
            " |      especially in regression.\n",
            " |  \n",
            " |      - If int, then consider `min_samples_leaf` as the minimum number.\n",
            " |      - If float, then `min_samples_leaf` is a fraction and\n",
            " |        `ceil(min_samples_leaf * n_samples)` are the minimum\n",
            " |        number of samples for each node.\n",
            " |  \n",
            " |      .. versionchanged:: 0.18\n",
            " |         Added float values for fractions.\n",
            " |  \n",
            " |  min_weight_fraction_leaf : float, default=0.0\n",
            " |      The minimum weighted fraction of the sum total of weights (of all\n",
            " |      the input samples) required to be at a leaf node. Samples have\n",
            " |      equal weight when sample_weight is not provided.\n",
            " |  \n",
            " |  max_features : {\"auto\", \"sqrt\", \"log2\"}, int or float, default=\"auto\"\n",
            " |      The number of features to consider when looking for the best split:\n",
            " |  \n",
            " |      - If int, then consider `max_features` features at each split.\n",
            " |      - If float, then `max_features` is a fraction and\n",
            " |        `round(max_features * n_features)` features are considered at each\n",
            " |        split.\n",
            " |      - If \"auto\", then `max_features=sqrt(n_features)`.\n",
            " |      - If \"sqrt\", then `max_features=sqrt(n_features)` (same as \"auto\").\n",
            " |      - If \"log2\", then `max_features=log2(n_features)`.\n",
            " |      - If None, then `max_features=n_features`.\n",
            " |  \n",
            " |      Note: the search for a split does not stop until at least one\n",
            " |      valid partition of the node samples is found, even if it requires to\n",
            " |      effectively inspect more than ``max_features`` features.\n",
            " |  \n",
            " |  max_leaf_nodes : int, default=None\n",
            " |      Grow trees with ``max_leaf_nodes`` in best-first fashion.\n",
            " |      Best nodes are defined as relative reduction in impurity.\n",
            " |      If None then unlimited number of leaf nodes.\n",
            " |  \n",
            " |  min_impurity_decrease : float, default=0.0\n",
            " |      A node will be split if this split induces a decrease of the impurity\n",
            " |      greater than or equal to this value.\n",
            " |  \n",
            " |      The weighted impurity decrease equation is the following::\n",
            " |  \n",
            " |          N_t / N * (impurity - N_t_R / N_t * right_impurity\n",
            " |                              - N_t_L / N_t * left_impurity)\n",
            " |  \n",
            " |      where ``N`` is the total number of samples, ``N_t`` is the number of\n",
            " |      samples at the current node, ``N_t_L`` is the number of samples in the\n",
            " |      left child, and ``N_t_R`` is the number of samples in the right child.\n",
            " |  \n",
            " |      ``N``, ``N_t``, ``N_t_R`` and ``N_t_L`` all refer to the weighted sum,\n",
            " |      if ``sample_weight`` is passed.\n",
            " |  \n",
            " |      .. versionadded:: 0.19\n",
            " |  \n",
            " |  bootstrap : bool, default=True\n",
            " |      Whether bootstrap samples are used when building trees. If False, the\n",
            " |      whole dataset is used to build each tree.\n",
            " |  \n",
            " |  oob_score : bool, default=False\n",
            " |      Whether to use out-of-bag samples to estimate the generalization score.\n",
            " |      Only available if bootstrap=True.\n",
            " |  \n",
            " |  n_jobs : int, default=None\n",
            " |      The number of jobs to run in parallel. :meth:`fit`, :meth:`predict`,\n",
            " |      :meth:`decision_path` and :meth:`apply` are all parallelized over the\n",
            " |      trees. ``None`` means 1 unless in a :obj:`joblib.parallel_backend`\n",
            " |      context. ``-1`` means using all processors. See :term:`Glossary\n",
            " |      <n_jobs>` for more details.\n",
            " |  \n",
            " |  random_state : int, RandomState instance or None, default=None\n",
            " |      Controls both the randomness of the bootstrapping of the samples used\n",
            " |      when building trees (if ``bootstrap=True``) and the sampling of the\n",
            " |      features to consider when looking for the best split at each node\n",
            " |      (if ``max_features < n_features``).\n",
            " |      See :term:`Glossary <random_state>` for details.\n",
            " |  \n",
            " |  verbose : int, default=0\n",
            " |      Controls the verbosity when fitting and predicting.\n",
            " |  \n",
            " |  warm_start : bool, default=False\n",
            " |      When set to ``True``, reuse the solution of the previous call to fit\n",
            " |      and add more estimators to the ensemble, otherwise, just fit a whole\n",
            " |      new forest. See :term:`the Glossary <warm_start>`.\n",
            " |  \n",
            " |  class_weight : {\"balanced\", \"balanced_subsample\"}, dict or list of dicts,             default=None\n",
            " |      Weights associated with classes in the form ``{class_label: weight}``.\n",
            " |      If not given, all classes are supposed to have weight one. For\n",
            " |      multi-output problems, a list of dicts can be provided in the same\n",
            " |      order as the columns of y.\n",
            " |  \n",
            " |      Note that for multioutput (including multilabel) weights should be\n",
            " |      defined for each class of every column in its own dict. For example,\n",
            " |      for four-class multilabel classification weights should be\n",
            " |      [{0: 1, 1: 1}, {0: 1, 1: 5}, {0: 1, 1: 1}, {0: 1, 1: 1}] instead of\n",
            " |      [{1:1}, {2:5}, {3:1}, {4:1}].\n",
            " |  \n",
            " |      The \"balanced\" mode uses the values of y to automatically adjust\n",
            " |      weights inversely proportional to class frequencies in the input data\n",
            " |      as ``n_samples / (n_classes * np.bincount(y))``\n",
            " |  \n",
            " |      The \"balanced_subsample\" mode is the same as \"balanced\" except that\n",
            " |      weights are computed based on the bootstrap sample for every tree\n",
            " |      grown.\n",
            " |  \n",
            " |      For multi-output, the weights of each column of y will be multiplied.\n",
            " |  \n",
            " |      Note that these weights will be multiplied with sample_weight (passed\n",
            " |      through the fit method) if sample_weight is specified.\n",
            " |  \n",
            " |  ccp_alpha : non-negative float, default=0.0\n",
            " |      Complexity parameter used for Minimal Cost-Complexity Pruning. The\n",
            " |      subtree with the largest cost complexity that is smaller than\n",
            " |      ``ccp_alpha`` will be chosen. By default, no pruning is performed. See\n",
            " |      :ref:`minimal_cost_complexity_pruning` for details.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  max_samples : int or float, default=None\n",
            " |      If bootstrap is True, the number of samples to draw from X\n",
            " |      to train each base estimator.\n",
            " |  \n",
            " |      - If None (default), then draw `X.shape[0]` samples.\n",
            " |      - If int, then draw `max_samples` samples.\n",
            " |      - If float, then draw `max_samples * X.shape[0]` samples. Thus,\n",
            " |        `max_samples` should be in the interval `(0.0, 1.0]`.\n",
            " |  \n",
            " |      .. versionadded:: 0.22\n",
            " |  \n",
            " |  Attributes\n",
            " |  ----------\n",
            " |  base_estimator_ : DecisionTreeClassifier\n",
            " |      The child estimator template used to create the collection of fitted\n",
            " |      sub-estimators.\n",
            " |  \n",
            " |  estimators_ : list of DecisionTreeClassifier\n",
            " |      The collection of fitted sub-estimators.\n",
            " |  \n",
            " |  classes_ : ndarray of shape (n_classes,) or a list of such arrays\n",
            " |      The classes labels (single output problem), or a list of arrays of\n",
            " |      class labels (multi-output problem).\n",
            " |  \n",
            " |  n_classes_ : int or list\n",
            " |      The number of classes (single output problem), or a list containing the\n",
            " |      number of classes for each output (multi-output problem).\n",
            " |  \n",
            " |  n_features_ : int\n",
            " |      The number of features when ``fit`` is performed.\n",
            " |  \n",
            " |      .. deprecated:: 1.0\n",
            " |          Attribute `n_features_` was deprecated in version 1.0 and will be\n",
            " |          removed in 1.2. Use `n_features_in_` instead.\n",
            " |  \n",
            " |  n_features_in_ : int\n",
            " |      Number of features seen during :term:`fit`.\n",
            " |  \n",
            " |      .. versionadded:: 0.24\n",
            " |  \n",
            " |  feature_names_in_ : ndarray of shape (`n_features_in_`,)\n",
            " |      Names of features seen during :term:`fit`. Defined only when `X`\n",
            " |      has feature names that are all strings.\n",
            " |      .. versionadded:: 1.0\n",
            " |  \n",
            " |  n_outputs_ : int\n",
            " |      The number of outputs when ``fit`` is performed.\n",
            " |  \n",
            " |  feature_importances_ : ndarray of shape (n_features,)\n",
            " |      The impurity-based feature importances.\n",
            " |      The higher, the more important the feature.\n",
            " |      The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance.\n",
            " |  \n",
            " |      Warning: impurity-based feature importances can be misleading for\n",
            " |      high cardinality features (many unique values). See\n",
            " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            " |  \n",
            " |  oob_score_ : float\n",
            " |      Score of the training dataset obtained using an out-of-bag estimate.\n",
            " |      This attribute exists only when ``oob_score`` is True.\n",
            " |  \n",
            " |  oob_decision_function_ : ndarray of shape (n_samples, n_classes) or             (n_samples, n_classes, n_outputs)\n",
            " |      Decision function computed with out-of-bag estimate on the training\n",
            " |      set. If n_estimators is small it might be possible that a data point\n",
            " |      was never left out during the bootstrap. In this case,\n",
            " |      `oob_decision_function_` might contain NaN. This attribute exists\n",
            " |      only when ``oob_score`` is True.\n",
            " |  \n",
            " |  See Also\n",
            " |  --------\n",
            " |  sklearn.tree.DecisionTreeClassifier : A decision tree classifier.\n",
            " |  sklearn.ensemble.ExtraTreesClassifier : Ensemble of extremely randomized\n",
            " |      tree classifiers.\n",
            " |  \n",
            " |  Notes\n",
            " |  -----\n",
            " |  The default values for the parameters controlling the size of the trees\n",
            " |  (e.g. ``max_depth``, ``min_samples_leaf``, etc.) lead to fully grown and\n",
            " |  unpruned trees which can potentially be very large on some data sets. To\n",
            " |  reduce memory consumption, the complexity and size of the trees should be\n",
            " |  controlled by setting those parameter values.\n",
            " |  \n",
            " |  The features are always randomly permuted at each split. Therefore,\n",
            " |  the best found split may vary, even with the same training data,\n",
            " |  ``max_features=n_features`` and ``bootstrap=False``, if the improvement\n",
            " |  of the criterion is identical for several splits enumerated during the\n",
            " |  search of the best split. To obtain a deterministic behaviour during\n",
            " |  fitting, ``random_state`` has to be fixed.\n",
            " |  \n",
            " |  References\n",
            " |  ----------\n",
            " |  .. [1] L. Breiman, \"Random Forests\", Machine Learning, 45(1), 5-32, 2001.\n",
            " |  \n",
            " |  Examples\n",
            " |  --------\n",
            " |  >>> from sklearn.ensemble import RandomForestClassifier\n",
            " |  >>> from sklearn.datasets import make_classification\n",
            " |  >>> X, y = make_classification(n_samples=1000, n_features=4,\n",
            " |  ...                            n_informative=2, n_redundant=0,\n",
            " |  ...                            random_state=0, shuffle=False)\n",
            " |  >>> clf = RandomForestClassifier(max_depth=2, random_state=0)\n",
            " |  >>> clf.fit(X, y)\n",
            " |  RandomForestClassifier(...)\n",
            " |  >>> print(clf.predict([[0, 0, 0, 0]]))\n",
            " |  [1]\n",
            " |  \n",
            " |  Method resolution order:\n",
            " |      RandomForestClassifier\n",
            " |      ForestClassifier\n",
            " |      sklearn.base.ClassifierMixin\n",
            " |      BaseForest\n",
            " |      sklearn.base.MultiOutputMixin\n",
            " |      sklearn.ensemble._base.BaseEnsemble\n",
            " |      sklearn.base.MetaEstimatorMixin\n",
            " |      sklearn.base.BaseEstimator\n",
            " |      builtins.object\n",
            " |  \n",
            " |  Methods defined here:\n",
            " |  \n",
            " |  __init__(self, n_estimators=100, *, criterion='gini', max_depth=None, min_samples_split=2, min_samples_leaf=1, min_weight_fraction_leaf=0.0, max_features='auto', max_leaf_nodes=None, min_impurity_decrease=0.0, bootstrap=True, oob_score=False, n_jobs=None, random_state=None, verbose=0, warm_start=False, class_weight=None, ccp_alpha=0.0, max_samples=None)\n",
            " |      Initialize self.  See help(type(self)) for accurate signature.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes defined here:\n",
            " |  \n",
            " |  __abstractmethods__ = frozenset()\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from ForestClassifier:\n",
            " |  \n",
            " |  predict(self, X)\n",
            " |      Predict class for X.\n",
            " |      \n",
            " |      The predicted class of an input sample is a vote by the trees in\n",
            " |      the forest, weighted by their probability estimates. That is,\n",
            " |      the predicted class is the one with highest mean probability\n",
            " |      estimate across the trees.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      y : ndarray of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The predicted classes.\n",
            " |  \n",
            " |  predict_log_proba(self, X)\n",
            " |      Predict class log-probabilities for X.\n",
            " |      \n",
            " |      The predicted class log-probabilities of an input sample is computed as\n",
            " |      the log of the mean predicted class probabilities of the trees in the\n",
            " |      forest.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  predict_proba(self, X)\n",
            " |      Predict class probabilities for X.\n",
            " |      \n",
            " |      The predicted class probabilities of an input sample are computed as\n",
            " |      the mean predicted class probabilities of the trees in the forest.\n",
            " |      The class probability of a single tree is the fraction of samples of\n",
            " |      the same class in a leaf.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      p : ndarray of shape (n_samples, n_classes), or a list of such arrays\n",
            " |          The class probabilities of the input samples. The order of the\n",
            " |          classes corresponds to that in the attribute :term:`classes_`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  score(self, X, y, sample_weight=None)\n",
            " |      Return the mean accuracy on the given test data and labels.\n",
            " |      \n",
            " |      In multi-label classification, this is the subset accuracy\n",
            " |      which is a harsh metric since you require for each sample that\n",
            " |      each label set be correctly predicted.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : array-like of shape (n_samples, n_features)\n",
            " |          Test samples.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          True labels for `X`.\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      score : float\n",
            " |          Mean accuracy of ``self.predict(X)`` wrt. `y`.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from sklearn.base.ClassifierMixin:\n",
            " |  \n",
            " |  __dict__\n",
            " |      dictionary for instance variables (if defined)\n",
            " |  \n",
            " |  __weakref__\n",
            " |      list of weak references to the object (if defined)\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from BaseForest:\n",
            " |  \n",
            " |  apply(self, X)\n",
            " |      Apply trees in the forest to X, return leaf indices.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      X_leaves : ndarray of shape (n_samples, n_estimators)\n",
            " |          For each datapoint x in X and for each tree in the forest,\n",
            " |          return the index of the leaf x ends up in.\n",
            " |  \n",
            " |  decision_path(self, X)\n",
            " |      Return the decision path in the forest.\n",
            " |      \n",
            " |      .. versionadded:: 0.18\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The input samples. Internally, its dtype will be converted to\n",
            " |          ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csr_matrix``.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      indicator : sparse matrix of shape (n_samples, n_nodes)\n",
            " |          Return a node indicator matrix where non zero elements indicates\n",
            " |          that the samples goes through the nodes. The matrix is of CSR\n",
            " |          format.\n",
            " |      \n",
            " |      n_nodes_ptr : ndarray of shape (n_estimators + 1,)\n",
            " |          The columns from indicator[n_nodes_ptr[i]:n_nodes_ptr[i+1]]\n",
            " |          gives the indicator value for the i-th estimator.\n",
            " |  \n",
            " |  fit(self, X, y, sample_weight=None)\n",
            " |      Build a forest of trees from the training set (X, y).\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      X : {array-like, sparse matrix} of shape (n_samples, n_features)\n",
            " |          The training input samples. Internally, its dtype will be converted\n",
            " |          to ``dtype=np.float32``. If a sparse matrix is provided, it will be\n",
            " |          converted into a sparse ``csc_matrix``.\n",
            " |      \n",
            " |      y : array-like of shape (n_samples,) or (n_samples, n_outputs)\n",
            " |          The target values (class labels in classification, real numbers in\n",
            " |          regression).\n",
            " |      \n",
            " |      sample_weight : array-like of shape (n_samples,), default=None\n",
            " |          Sample weights. If None, then samples are equally weighted. Splits\n",
            " |          that would create child nodes with net zero or negative weight are\n",
            " |          ignored while searching for a split in each node. In the case of\n",
            " |          classification, splits are also ignored if they would result in any\n",
            " |          single class carrying a negative weight in either child node.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : object\n",
            " |          Fitted estimator.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data descriptors inherited from BaseForest:\n",
            " |  \n",
            " |  feature_importances_\n",
            " |      The impurity-based feature importances.\n",
            " |      \n",
            " |      The higher, the more important the feature.\n",
            " |      The importance of a feature is computed as the (normalized)\n",
            " |      total reduction of the criterion brought by that feature.  It is also\n",
            " |      known as the Gini importance.\n",
            " |      \n",
            " |      Warning: impurity-based feature importances can be misleading for\n",
            " |      high cardinality features (many unique values). See\n",
            " |      :func:`sklearn.inspection.permutation_importance` as an alternative.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      feature_importances_ : ndarray of shape (n_features,)\n",
            " |          The values of this array sum to 1, unless all trees are single node\n",
            " |          trees consisting of only the root node, in which case it will be an\n",
            " |          array of zeros.\n",
            " |  \n",
            " |  n_features_\n",
            " |      DEPRECATED: Attribute `n_features_` was deprecated in version 1.0 and will be removed in 1.2. Use `n_features_in_` instead.\n",
            " |      \n",
            " |      Number of features when fitting the estimator.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.ensemble._base.BaseEnsemble:\n",
            " |  \n",
            " |  __getitem__(self, index)\n",
            " |      Return the index'th estimator in the ensemble.\n",
            " |  \n",
            " |  __iter__(self)\n",
            " |      Return iterator over estimators in the ensemble.\n",
            " |  \n",
            " |  __len__(self)\n",
            " |      Return the number of estimators in the ensemble.\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Data and other attributes inherited from sklearn.ensemble._base.BaseEnsemble:\n",
            " |  \n",
            " |  __annotations__ = {'_required_parameters': typing.List[str]}\n",
            " |  \n",
            " |  ----------------------------------------------------------------------\n",
            " |  Methods inherited from sklearn.base.BaseEstimator:\n",
            " |  \n",
            " |  __getstate__(self)\n",
            " |  \n",
            " |  __repr__(self, N_CHAR_MAX=700)\n",
            " |      Return repr(self).\n",
            " |  \n",
            " |  __setstate__(self, state)\n",
            " |  \n",
            " |  get_params(self, deep=True)\n",
            " |      Get parameters for this estimator.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      deep : bool, default=True\n",
            " |          If True, will return the parameters for this estimator and\n",
            " |          contained subobjects that are estimators.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      params : dict\n",
            " |          Parameter names mapped to their values.\n",
            " |  \n",
            " |  set_params(self, **params)\n",
            " |      Set the parameters of this estimator.\n",
            " |      \n",
            " |      The method works on simple estimators as well as on nested objects\n",
            " |      (such as :class:`~sklearn.pipeline.Pipeline`). The latter have\n",
            " |      parameters of the form ``<component>__<parameter>`` so that it's\n",
            " |      possible to update each component of a nested object.\n",
            " |      \n",
            " |      Parameters\n",
            " |      ----------\n",
            " |      **params : dict\n",
            " |          Estimator parameters.\n",
            " |      \n",
            " |      Returns\n",
            " |      -------\n",
            " |      self : estimator instance\n",
            " |          Estimator instance.\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hIyHXIOg7ugZ"
      },
      "source": [
        "### <font color='red'>2.1) Please try to change several hyperparameters in RandomForestClassifier to improve model performance </font>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YmHu71-P7ugZ"
      },
      "source": [
        "#### n_estimators"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mbEp6Doy7uga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "97e4b861-46c7-4cb4-b30e-424bd4148f2e"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 20)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.84760944 0.84994734 0.8445711 ]\n",
            "Random Forest Average Score:  0.8473759602448955\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OQ-pXY8-7uga",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66fcbe29-16ee-4599-fc93-b4b714d967eb"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 50)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86028383 0.86093888 0.85798672]\n",
            "Random Forest Average Score:  0.8597364763860255\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "U-n4P53o7ugb"
      },
      "source": [
        "#### max_depth"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pW_m8KC_7ugb",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "497f8e7a-a86c-42ac-9356-0944592e2c59"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 50, max_depth = 10)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.84109412 0.83903832 0.83733086]\n",
            "Random Forest Average Score:  0.8391544361231019\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9XyjgSyY7ugc",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f7760d7f-6b2c-4011-91c1-7e3bc985cc45"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 50, max_depth = 20)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86253749 0.86231759 0.86010393]\n",
            "Random Forest Average Score:  0.861653006727296\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C4J_MdEp7ugc"
      },
      "source": [
        "#### max_features"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Tg3QD56H7ugd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62bdfa0a-55ae-47e6-e0ff-ce25e702511c"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 50, max_depth = 20, max_features = 5)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86293286 0.86346103 0.86091553]\n",
            "Random Forest Average Score:  0.8624364719286058\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-bgJOIU27ugd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "76bbcc58-94ba-4bcf-d7c8-ed62cd23ba2d"
      },
      "source": [
        "from sklearn.ensemble import RandomForestClassifier\n",
        "rf = RandomForestClassifier(n_estimators = 50, max_depth = 20, max_features = 10)\n",
        "score = cross_val_score(rf, X_train, y_train, scoring='roc_auc', cv=3)\n",
        "print(score)\n",
        "print('Random Forest Average Score: ', score.mean())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[0.86312448 0.86263132 0.86036169]\n",
            "Random Forest Average Score:  0.8620391651920057\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rP1m26hr7ugd",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf4501f-7f50-43bc-da35-3cfb799e969a"
      },
      "source": [
        "rf.fit(X_train, y_train) # fit the final model"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RandomForestClassifier(max_depth=20, max_features=10, n_estimators=50)"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n6ZvMZO27uge"
      },
      "source": [
        "### <font color='red'>2.3) Save the final model </font>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ENrG3Arr7uge"
      },
      "source": [
        "import pickle"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "_irHcn9NjdXr",
        "outputId": "4f897122-a9b9-4026-c544-3faea3183915"
      },
      "source": [
        "os.getcwd() # << Cek current folder directory"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'/content/drive/My Drive/dataset_gclass'"
            ]
          },
          "metadata": {},
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KnEfaHIp7uge"
      },
      "source": [
        "import os\n",
        "os.chdir('/content/drive/My Drive/dataset_gclass') # << Ganti directory folder"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1TZnSorl7ugk"
      },
      "source": [
        "# export model as a pickle object\n",
        "filename = 'randomforest.pkl'\n",
        "pickle.dump(rf, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cn3IG5Md7ugk"
      },
      "source": [
        "filename = 'logisticregression.pkl'\n",
        "pickle.dump(lr, open(filename, 'wb'))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pjyf5Q-47ugk"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a4dUVHD07ugl"
      },
      "source": [
        "# 3) Exercise - Please be creative and try to improve model performance <font color='blue'> (20 min) </font>"
      ]
    }
  ]
}